{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f44d661",
   "metadata": {},
   "source": [
    "# 2.1 말뭉치, 토큰, 타입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294ed7e",
   "metadata": {},
   "source": [
    "고전이나 현대 모든 NLP 작업은 '말뭉치'라 부르는 텍스트 데이터로 시작합니다.\n",
    "\n",
    "원시 텍스트는 문자 시퀸스지만 일반적으로 문자를 \"토큰\"이라는 연속된 단위로 묶었을 때 유용합니다.\n",
    "\n",
    "머신러닝 분야에서는 메타데이터가 붙은 텍스트를 \"샘플\" 혹은 \"데이터 포인트\"라고 부릅니다. \n",
    "\n",
    "샘플의 모음인 말뭉치(아래 그림)은 \"데이터셋\" 이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51a4d6",
   "metadata": {},
   "source": [
    "<img src = \"2-1.jpg\" width = \"300px\" height = \"300px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501ee47",
   "metadata": {},
   "source": [
    "\"토큰화\"는 텍스트를 토큰으로 나누는 과정을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cc74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-1 텍스트 토큰화\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Mary, don't slap the green witch\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab73a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degress', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet = u\"Snow White and the Seven Degress #MakeAMovieCold@midnight:-)\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897e604",
   "metadata": {},
   "source": [
    "타입은 말뭉치에 등장하는 고유한 토큰입니다. 말뭉치에 있는 모든 타입의 집합이 어휘 사전 또는 어휘입니다.  \n",
    "단어는 \"내용어\"와 \"불용어\"로 구분됩니다. 관사와 전치사 같은 불용어는 대부분 내용어를 보충하는 문법적인 용도로 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342972d",
   "metadata": {},
   "source": [
    "# 2.2 유니그램, 바이그램, 트라이그램, ... , n-그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca11283",
   "metadata": {},
   "source": [
    "n-그램은 텍스트에 있는 고정길이(n)의 연속된 토큰 시퀀스입니다.  \n",
    "바이그램은 토큰 두개, 유니그램은 토큰 한개로 이루어집니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d65de89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['marry', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-2 텍스트에서 n-그램 만들기\n",
    "def n_grams(text, n):\n",
    "    '''\n",
    "    takes tokens or text, return a list of n-grams\n",
    "    '''\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "cleaned = ['marry', ',', \"n't\", 'slap', 'green', 'witch', '.']\n",
    "print(n_grams(cleaned, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740ce16",
   "metadata": {},
   "source": [
    "# 표제어와 어간"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167f38f",
   "metadata": {},
   "source": [
    "표제어(lemma)는 단어의 기본형입니다.  \n",
    "ex) flow, flew, flies, flown, flowing등 어미가 바뀌면서 여러 단어로 변형됩니다. 이런 모든 단어의 표제어는 fly입니다.  \n",
    "표제어 추출 -> 토큰을 표제어로 바꾸어 벡터 표현의 차원을 줄이는 방법."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f27ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-3 표제어 추출: 단어를 표제어로 바꿉니다.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"he was running late\") # 왜 u를 앞에 붙이는지 의문..\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e9a02e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he --> he\n",
      "was --> be\n",
      "running --> run\n",
      "late --> late\n"
     ]
    }
   ],
   "source": [
    "# u 제거 버젼\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"he was running late\") # 왜 u를 앞에 붙이는지 의문..\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711006e",
   "metadata": {},
   "source": [
    "token.lemma_ : 표제어 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4c1f1",
   "metadata": {},
   "source": [
    "어간 추출은 표제어 추출 대신에 사용하는 축소 기법입니다. 수동으로 만든 규칙을 사용해 단어의 끝을 잘라 어간이라는 공통 형태로 축소합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba031c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "s=PorterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "198edb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "l=LancasterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183c4a0",
   "metadata": {},
   "source": [
    "어떤 규칙을 가진 어간 추출기를 사용하냐에 따라 어간추출 방법이 달라집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab0eca",
   "metadata": {},
   "source": [
    "# 2.4 문장과 문서 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eec43a",
   "metadata": {},
   "source": [
    "TF와 TF-IDF 표현이 문서나 문장같은 긴 텍스트 뭉치를 분류하는 데 유용합니다.  \n",
    "토픽 레이블 할당, 리뷰의 감성 에측, 스팸 이메일 필터링, 언어 식별, 이메일 분류 같은 작업은 지도 학습 기반의 문서 분류 문제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d16a23",
   "metadata": {},
   "source": [
    "# 2.5 단어 분류하기: 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25d79f",
   "metadata": {},
   "source": [
    "문서에 레이블을 할당하는 개념을 단어나 토큰으로 확장할 수 있습니다.  \n",
    "단어 분류 작업의 에로는 품사(pos)태깅이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09b50dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> PROPN\n",
      "slapped --> VERB\n",
      "the --> DET\n",
      "green --> PROPN\n",
      "witch --> NOUN\n",
      ". --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-4 품사 태깅\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "for token in doc:\n",
    "    print(\"{} --> {}\".format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f180947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> PROPN\n",
      "slapped --> VERB\n",
      "the --> DET\n",
      "green --> PROPN\n",
      "witch --> NOUN\n",
      ". --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# u 제거 버전\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "for token in doc:\n",
    "    print(\"{} --> {}\".format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca347c7",
   "metadata": {},
   "source": [
    "token.pos_ : 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89b027d",
   "metadata": {},
   "source": [
    "# 2.6 청크 나누기와 개체명 인식\n",
    "\n",
    "명사, 동사. 형용사 같은 문법 요소를 구성된 고차원의 단위를 유도하는 것 -> 청크 나누기, 부분 구문 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93f0f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary --> NP\n",
      "the green witch --> NP\n"
     ]
    }
   ],
   "source": [
    "# 코드 2-5 명사구(NP) 부분 구문 분석\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"Mary slapped the green witch.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"{} --> {}\".format(chunk, chunk.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2db30",
   "metadata": {},
   "source": [
    "또 다른 유용한 단위는 개체명 입니다.  \n",
    "개체명은 사람, 장소, 회사, 약 이름과 같은 실제 세상의 개념을 의미하는 문자열 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b577929",
   "metadata": {},
   "source": [
    "# 2.7 문장 구조  \n",
    "\n",
    "구 사이의 관계를 파악하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b86fae",
   "metadata": {},
   "source": [
    "구성 구문 분석\n",
    "\n",
    "<img src = \"2-3.jpg\" width = \"400px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e06ea0",
   "metadata": {},
   "source": [
    "의존 구문 분석\n",
    "\n",
    "<img src = \"2-4.jpg\" width = \"400px\" height = \"400px\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
