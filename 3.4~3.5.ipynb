{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d8745c",
   "metadata": {},
   "source": [
    "# 3.4 지도 학습 훈련 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2780c",
   "metadata": {},
   "source": [
    "지도학습에는 모델, 손실 함수, 훈련 데이터, 최적화 알고리즘이 필요합니다.  \n",
    "지도학습의 훈련 데이터는 샘플과 타깃의 쌍입니다.  \n",
    "모델은 샘플에 대한 예측을 계산하고, 손실 함수는 타깃과 비교하여 오차를 측정합니다.  \n",
    "훈련 목표는 그레이디언트 기반의 최적화 알고리즘으로 모델의 파라미터를 조정하여 가능한 한 낮은 손실을 내는 것 입니다.  \n",
    "\n",
    "한 클래스 포인트를 다른 클래스와 구별하는 것을 \"결정 경계\", 또는 \"초평면\" 이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d75eff",
   "metadata": {},
   "source": [
    "# 3.4.1 예제 데이터 만들기\n",
    "\n",
    "머신러닝에서는 알고리즘을 이해하고자 할 때 일반적으로 설명하기 쉬운 성질의 합성 데이터를 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9703b27",
   "metadata": {},
   "source": [
    "# 3.4.2 모델 선택\n",
    "\n",
    "여기서 사용할 모델은 퍼셉트론입니다. 퍼셉트론은 어떤 크기의 입력도 다룰 수 있습니다.  \n",
    "일반적인 모델 구축 과정에서 입력 크기는 문제와 데이터에 따라 결정됩니다.\n",
    "\n",
    "퍼셉트론의 활성화 함수가 시그모이드 이므로 퍼센트론의 출력은 데이터 포인트(x)가 클래스 1일 확률입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6643e9",
   "metadata": {},
   "source": [
    "<img src = \"3-2-1.jpg\" width = \"400px\" height = \"400px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64f55b",
   "metadata": {},
   "source": [
    "# 3.4.3 확률을 클래스로 변환하기\n",
    "\n",
    "<img src = \"3-2-2.jpg\" width = \"400px\" height = \"400px\"></img>\n",
    "\n",
    "일반적으로 결정 경계는  0.5로 지정합니다. 하지만 실전에서 만족스러운 분류 정밀도를 얻으려면 (검증 데이터셋으로) 이 하이퍼파라미터를 수정해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41ac78",
   "metadata": {},
   "source": [
    "# 3.4.4 옵티마이저 선택\n",
    "\n",
    "모델이 예측을 만들고 손실 함수가 예측과 타깃 사이의 오차를 측정하면 옵티마이저가 이 오차 신호를 사용해 모델의 가중치를 업데이트 합니다.  \n",
    "\"학습률\" 이라고 부르는 이 하이퍼파라미터는 오차 신호가 가중치 업데이트에 영향을 얼마나 미치는지 조절합니다.  \n",
    "학습률이 크면 가중치가 크게 바뀌고, 수렴에 영향을 미칠 수 있습니다. 학습률이 너무 작으면 훈련 진행 속도가 심하게 느려질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03eab542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 3-1 파이토치로 구현한 퍼셉트론\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\"퍼셉트론은 하나의 선형 층입니다\"\"\"\n",
    "    def _init_(self, input_dim):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            imput_dim (int): 입력 특성의 크기\n",
    "        \"\"\"\n",
    "        super(Perceptron, self)._init_()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        \"\"\"퍼셉트론의 정방향 계산\n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 데이터 텐서\n",
    "                x_in.shape는 (batch, num_features)입니다.\n",
    "        반환값:\n",
    "            결과 텐서. tensor.shape는 (batch,) 입니다.\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6713babd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18172\\1631292150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mperceptron\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mbce_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpercetron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'input_dim'"
     ]
    }
   ],
   "source": [
    "# 코드 3-10 Adam 옵티마이저 준비\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "input_dim = 2\n",
    "lr = 0.001\n",
    "\n",
    "perceptron = Perceptron(input_dim=input_dim)\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = optim.Adam(params=percetron.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170fcdd",
   "metadata": {},
   "source": [
    "# 3.4.6 모두 합치기: 그레이디언트 기반의 지도 학습\n",
    "\n",
    "학습은 손실 계산에서 시작합니다. 즉, 모델의 예측이 타깃에서 얼마나 멀리 떨어져 있는지 측정합니다.  \n",
    "손실의 그레이디언트는 모델 파라미터를 얼마나 많이 바꿔야 하는지 나타내는 신호가 됩니다.  \n",
    "각 파라미터의 그레이디언트는 이 파라미터에 대한 손실값의 순간 변화율을 의미합니다.\n",
    "\n",
    "그레이디언트 업데이트 알고리즘이 어떻게 동작할까요?\n",
    "1. 모델(perceptron) 객체 안에 저장된 그레이디언트와 같은 부가 정보를 zero_grad()함수로 초기화 합니다.\n",
    "2. 모델이 입력 데이터(x_data)에 대한 출력 (y_pred)을 계산합니다.\n",
    "3. 모델 출력(y_pred)과 기대하는 타깃(y_target)을 비교해 손실을 계산합니다. 이것이 지도학습 훈련의 지도에 해당합니다.\n",
    "4. 파이토치 손실 객체(criterion)에는 backward() 메서드가 있습니다. 이 메서드를 사용해 계산 그래프를 거슬러 손실을 반복해서 전파하고 각 파라미터에 대한 그레이디언트를 계산합니다.\n",
    "5. 마지막으로 옵티마이저(opt)는 step() 함수로 파라미터에 그레이디언트를 업데이트 하는 방법을 지시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebc692",
   "metadata": {},
   "source": [
    "에포크 : 완전한 훈련 반복 한 번  \n",
    "에포크 당 배치 개수가 데이터셋의 배치 개수와 같다면, 에포크가 데이터셋에 대한 완전한 반복 한 번이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e13eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18172\\2708033222.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 코드 3-11 퍼셉트론과 이진 분류를 위한 지도 학습 훈련 반복\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 각 에포크는 전체 훈련 데이터를 사용합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0meqoch_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# 내부 반복은 데이터셋에 있는 배치에 대해 수행됩니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batchs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# 코드 3-11 퍼셉트론과 이진 분류를 위한 지도 학습 훈련 반복\n",
    "# 각 에포크는 전체 훈련 데이터를 사용합니다.\n",
    "for eqoch_i in range(n_epochs):\n",
    "    # 내부 반복은 데이터셋에 있는 배치에 대해 수행됩니다.\n",
    "    for batch_i in range(n_batchs): \n",
    "        \n",
    "        # 0단계: 데이터 가져오기\n",
    "        x_data, t_target = get_toy_data(batch_size)\n",
    "        \n",
    "        # 1단계: 그레이디언트 초기화\n",
    "        perceptron.zero_grad()\n",
    "        \n",
    "        # 2단계: 모델의 정방향 계산 수행하기\n",
    "        y_pred = perceptron(x_data, apply_sigmoid=True)\n",
    "        \n",
    "        # 3단계: 최적하려는 손실 계산하기\n",
    "        loss = bce_loss(y_pred, y_target)\n",
    "        \n",
    "        # 4단계: 손실 신호를 거꾸로 전파하기\n",
    "        loss.backward()\n",
    "         \n",
    "        # 5단계: 옵티마이저로 업데이트하기\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f11a50",
   "metadata": {},
   "source": [
    "# 3.5 부가적인 훈련 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24120b99",
   "metadata": {},
   "source": [
    "# 3.5.1 모델 성능 올바르게 측정하기: 평가 지표\n",
    "\n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99DC064C5BE056CE10\" width = \"400px\" height = \"400px\"></img>\n",
    "\n",
    "True Positive(TP) : 실제 True인 정답을 True 라고 예측 (정답)  \n",
    "False Positive(FP) : 실제 False인 정답을 True 라고 예측 (오답)  \n",
    "False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)  \n",
    "True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc953386",
   "metadata": {},
   "source": [
    "# 3.5.2 모델 성능 올바르게 측정하기: 데이터 분할\n",
    "\n",
    "<img src = \"https://velog.velcdn.com/images/pppanghyun/post/2a39921c-be87-4c53-ba92-3e5c4c06699d/image.png\" width = \"400px\" height = \"400px\"></img>\n",
    "\n",
    "K-fold 교차검증이란, \"재표본을 추출\"해서 모델을 학습하는 방법입니다.\n",
    "1. 우선 train 데이터를 'k'개로 나눕니다.\n",
    "2. 이후 한 번씩 돌아가면서(iteration) k개 만큼 나눈 데이터중 한 개를 test data로 사용하고 나머지는 train으로 사용합니다.  \n",
    "이런경우 모든 데이터를 train에 사용할 수 있어 모델의 오버피팅을 방지하고 한정된 데이터셋으로도 안정적인 학습이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1619687",
   "metadata": {},
   "source": [
    "# 3.5.3 훈련 중지 시점 파악하기\n",
    "\n",
    "올바른 모델 성능을 측정하는 이유는 이 값을 사용해 훈련을 멈출 때를 결정하기 위해서 입니다.\n",
    "\n",
    "가장 널리 사용하는 방법은 \"조기종료\" 입니다.  \n",
    "조기 종료는 에포크마다 검증 데이터셋에 대한 성능을 기록하고 이 성능이 더는 좋아지지 않을 때를 감지합니다.\n",
    "\n",
    "성능이 계속 좋아지지 않으면 훈련을 종료합니다. 훈련을 종료하기전에 기다리는 에포크 횟수를 \"인내\" 라고 합니다.  \n",
    "일반적으로 모델이 어떤 데이터셋에서 개선되지 않는 지점을 \"수렴\"된 곳이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7416818",
   "metadata": {},
   "source": [
    "# 3.5.4 최적의 하이퍼파라미터 찾기\n",
    "\n",
    "하이퍼파라미터는 모델의 파라미터 개수와 값에 영향을 미치는 모든 모델 선정입니다.  \n",
    "모델 훈련 방식을 결정하는 선택 옵션에는 손실함수, 옵티마이저, 옵티마이저의 학습률, 층 크기, 조기 종료하기전에 인내할 에포크 수, 다양한 규제 방법 등이 포함됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d9264",
   "metadata": {},
   "source": [
    "# 3.5.5 규제\n",
    "\n",
    "### 1. L1 규제  \n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F3UYxP%2Fbtq92R7IFel%2Fftu7d7b2dAYwvqOfROFTHk%2Fimg.png\" width = \"400px\" height = \"400px\"></img>  \n",
    "앞부분은 실제값과 예측값을 대상으로 loss함수를 구한 값이고, 뒤의 부분이 L1 norm 이다. 람다값은 설정해야할 하이퍼파라미터이고, 1부터 n가지의 가중치의 절대값을 모두 더한 후 람다값을 곱한 값이 L1 norm이 된다.\n",
    "\n",
    "Loss는 전체적으로 작아져야하기 때문에 람다값이 커지면 커질 수록 가중치 합은 줄어들 것 이다.(람다값이 커질수록 기울기가 작아짐)\n",
    "\n",
    "L1 norm을 미분하면 가중치 부분은 사라지고 람다값에 w의 부호를 곱한 값이 된다.  \n",
    "grad += lambda * np.sign(w)\n",
    "\n",
    "### 2. L2 규제  \n",
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FtfiG0%2Fbtq97ndWqpe%2FTXkMJpyH0f2x5F3DKIRpk1%2Fimg.png\" width = \"400px\" height = \"400px\"></img>  \n",
    "L1 규제의 경우 L1 norm을 미분하면서 가중치 부분이 부호만 남기고 사라지기 때문에 그레이디언트 값에 가중치를 반영할 수는 없다.  \n",
    "l2 규제가 검증데이터의 손실을 잘 억제하면서 가중치를 완전히 0으로 만들지 않기 때문에 l1 보다는 l2를 많이 사용하는 추세이다.  \n",
    "\n",
    "가중치를 제곱한 값의 합에 람다값을 곱한 값을 사용한다.  \n",
    "l2 규제를 gradient값에 반영해 주기위해 미분하면 2lambda * w가 남는다. 따라서 해당 값을 구해진 그레디언트 값에 더하면 규제를 반영할 수 있다.  \n",
    "grad += 2lambda * w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
